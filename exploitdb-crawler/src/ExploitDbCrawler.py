import logging
import os
import time
import requests
from bs4 import BeautifulSoup


class ExploitDbCrawler:
    def __init__(self,
                 path_storage='/Users/dravalico/PycharmProjects/crawlers/exploitdb-crawler/data',
                 request_timeout=20,
                 update_interval=3600,
                 retry_interval=60):
        self.path_storage = path_storage
        self.request_timeout = request_timeout
        self.update_interval = update_interval
        self.retry_interval = retry_interval
        log_format = f'[%(levelname)s at %(asctime)s] %(message)s'
        logging.basicConfig(level=logging.INFO, format=log_format, datefmt="%Y-%m-%d %H:%M:%S")

    def run(self):
        if not os.path.exists(self.path_storage):
            os.makedirs(self.path_storage)
        while True:
            self.download_data()
            time.sleep(self.update_interval)

    def retrieve_last_edb_id(self):
        pass

    def download_data(self, edb_id_from=1):
        endpoint_exploits_info = 'https://www.exploit-db.com/exploits/'
        endpoint_exploits_code = 'https://www.exploit-db.com/raw/'
        for i in range(edb_id_from, 100000):
            url = endpoint_exploits_code + str(i)
            try:
                response = requests.get(url, timeout=self.request_timeout)
                print(response)
                if response.status_code == 200:
                    soup = BeautifulSoup(response.content, 'html.parser')
                    text_content = soup.get_text()
                    print(text_content)
            except:
                pass
