import logging
import os
import time
import requests
import xml.etree.ElementTree as ET
from bs4 import BeautifulSoup
import json


class ExploitDbCrawler:
    def __init__(self,
                 path_storage='/usr/src/data',
                 request_timeout=10,
                 update_interval=86400,
                 retry_interval=1200):
        self.path_storage = path_storage
        self.request_timeout = request_timeout
        self.update_interval = update_interval
        self.retry_interval = retry_interval
        log_format = f'[%(levelname)s at %(asctime)s] %(message)s'
        logging.basicConfig(level=logging.INFO, format=log_format, datefmt="%Y-%m-%d %H:%M:%S")

    def run(self):
        logging.info("Crawler up")
        if not os.path.exists(self.path_storage):
            os.makedirs(self.path_storage)
        endpoint_exploits_info = 'https://www.exploit-db.com/exploits/'
        endpoint_exploits_code = 'https://www.exploit-db.com/raw/'
        interval_between_requests = 10
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) '
                          'Chrome/58.0.3029.110 Safari/537.3'}
        while True:
            is_exception_or_too_many_request = False
            try:
                logging.info(f'Starting the cycle...')
                last_online_id = self.retrieve_last_edb_id_from_rss()
                last_local_id = self.retrieve_last_local_edb_id()
                logging.info(f'The last local edb-id is {last_local_id} and the last online edb-id is {last_online_id}')
                for edb_id in range(last_local_id + 1, last_online_id + 1):
                    logging.info(f'Request for edb-id={edb_id}')
                    json_data = {}
                    response = requests.get(endpoint_exploits_info + str(edb_id),
                                            timeout=self.request_timeout,
                                            headers=headers)
                    if response.status_code == 200:
                        soup = BeautifulSoup(response.text, 'html.parser')
                        info_titles = soup.find_all('h4', class_='info-title')
                        info_titles = [e.text.strip().replace(':', '') for e in info_titles]
                        stats_titles = soup.find_all('h6', class_='stats-title')
                        stats_titles = [e.text.strip() for e in stats_titles]
                        for i, s in zip(info_titles, stats_titles):
                            json_data[f'{i}'] = s
                        time.sleep(interval_between_requests)
                        response = requests.get(endpoint_exploits_code + str(edb_id),
                                                timeout=self.request_timeout,
                                                headers=headers)
                        if response.status_code == 200:
                            json_data['code'] = response.text
                            logging.info(f'Data obtained for ebd_id={edb_id}')
                            self.save_edb_id_data(json_data)
                            logging.info(f'Data saved for ebd_id={edb_id}')
                        time.sleep(interval_between_requests)
                    else:
                        logging.error(f'Cannot obtain data for ebd_id={edb_id}, status code={response.status_code}')
                        if response.status_code == 403 or response.status_code == 429:
                            is_exception_or_too_many_request = True
                            break
            except Exception as e:
                logging.exception(e)
                is_exception_or_too_many_request = True
            if is_exception_or_too_many_request:
                logging.info('Going to sleep due to too many requests or an exception')
                time.sleep(self.retry_interval)
            else:
                logging.info('Going to sleep due to normal stand-by mode')
                time.sleep(self.update_interval)

    def retrieve_last_edb_id_from_rss(self):
        rss_url = 'https://www.exploit-db.com/rss.xml'
        try:
            response = requests.get(rss_url, timeout=self.request_timeout)
            if response.status_code == 200:
                xml_data = ET.fromstring(response.content)
                return int(xml_data.find(".//guid").text.split('/')[-1])
            else:
                raise RuntimeError(f'Cannot retrieve rss feed. Status code: {response.status_code}')
        except:
            raise RuntimeError('Cannot retrieve rss feed')

    def retrieve_last_local_edb_id(self):
        filenames = []
        for root_folder, folders, files in os.walk(self.path_storage):
            for file in files:
                edb_id = os.path.join(root_folder, file)
                try:
                    edb_id = edb_id.split('/')[-1].replace('.json', '')
                    filenames.append(int(edb_id))
                except:
                    pass
        return max(filenames) if filenames != [] else 0

    def save_edb_id_data(self, json_data):
        date = json_data['Date'].split('-')
        year = date[0]
        month = date[1]
        year_path = os.path.join(self.path_storage, year)
        if not os.path.exists(year_path):
            os.makedirs(year_path)
        month_path = os.path.join(year_path, month)
        if not os.path.exists(month_path):
            os.makedirs(month_path)
        with open(os.path.join(month_path, f'{json_data["EDB-ID"]}.json'), 'w', encoding='utf-8') as file:
            file.write(json.dumps(json_data))
