import datetime
import json
import logging
import os
import time
import requests
import xml.etree.ElementTree as ET


class CVECrawler:
    def __init__(self,
                 path_storage='/usr/src/data',
                 request_timeout=10,
                 update_interval=21600,
                 retry_interval=600,
                 n_years_redownload=4):
        self.path_storage = path_storage
        self.request_timeout = request_timeout
        self.update_interval = update_interval
        self.retry_interval = retry_interval
        self.n_years_redownload = n_years_redownload
        log_format = f'[%(levelname)s at %(asctime)s] %(message)s'
        logging.basicConfig(level=logging.INFO, format=log_format, datefmt="%Y-%m-%d %H:%M:%S")

    def run(self):
        if not os.path.exists(self.path_storage):
            os.makedirs(self.path_storage)
        while True:
            endpoint_cve = 'https://cveawg.mitre.org/api/cve/'
            for year in range(1999, int(datetime.date.today().year) + 1):
                try:
                    published_cve = self.retrieve_published_cve_per_year(year)
                    local_cve = self.retrieve_saved_cve_list(year)
                    cve_to_download = list(set(published_cve) ^ set(local_cve))
                    cve_to_download.sort()
                    for cve in cve_to_download:
                        url = endpoint_cve + cve
                        try:
                            response = requests.get(url, timeout=self.request_timeout)
                            if response.status_code == 200:
                                response_json = response.json()
                                complete_json = self.add_references_to_json(response_json)
                                self.save_cve_data(complete_json)
                                logging.info(f'Data obtained for {url.split("/")[-1]}')
                            elif response.status_code == 404:
                                logging.error(f'{url.split("/")[-1]} does not exists, error {response.status_code}')
                            elif response.status_code == 429:
                                time.sleep(self.retry_interval)
                            else:
                                logging.error(f'Cannot obtain data for {url.split("/")[-1]}')
                        except:
                            logging.exception(f'Error for {url.split("/")[-1]} during GET')
                except RuntimeError:
                    logging.error(f'Cannot obtain data for year {year}')
            time.sleep(self.update_interval)

    def retrieve_published_cve_per_year(self, year):
        xml_data = self.download_summary_per_year(year)
        published_cve = []
        for vulnerability_element in xml_data.findall(".//{http://www.icasi.org/CVRF/schema/vuln/1.1}Vulnerability"):
            notes = vulnerability_element.find("{http://www.icasi.org/CVRF/schema/vuln/1.1}Notes")
            is_published = any(
                note.get("Title") == 'Published' for note in
                notes.findall("{http://www.icasi.org/CVRF/schema/vuln/1.1}Note"))
            if is_published:
                cve = vulnerability_element.find("{http://www.icasi.org/CVRF/schema/vuln/1.1}CVE").text
                published_cve.append(cve)
        return published_cve

    def download_summary_per_year(self, year):
        summaries_path = 'cve_summaries_per_year'
        file_path = os.path.join(self.path_storage, summaries_path)
        if not os.path.exists(file_path):
            os.makedirs(file_path)
        file_path = os.path.join(file_path, f'{year}.xml')
        try:
            if year in [int(datetime.date.today().year) - i for i in range(0, self.n_years_redownload)]:
                os.remove(file_path)
            return ET.parse(file_path).getroot()
        except (FileNotFoundError, ET.ParseError) as e:
            logging.info(f'Downloading summary of {year}')
            all_items_cvrf_url = f'https://cve.mitre.org/data/downloads/allitems-cvrf-year-{year}.xml'
            try:
                response = requests.get(all_items_cvrf_url, timeout=self.request_timeout + 15)
                if response.status_code == 200:
                    with open(file_path, 'wb') as file:
                        file.write(response.content)
                    return ET.fromstring(response.content)
                else:
                    raise RuntimeError(f'Failed to download data for {year}. Status code: {response.status_code}')
            except:
                raise RuntimeError(f'Failed to download data for {year}')

    def retrieve_saved_cve_list(self, year):
        file_path = os.path.join(self.path_storage, str(year))
        filenames = []
        for root_folder, folders, files in os.walk(file_path):
            for file in files:
                cve = os.path.join(root_folder, file)
                try:
                    cve = cve.split('/')[-1].replace('.json', '')
                    split_cve = cve.split('-')
                    cve_begin = split_cve[0] + '-' + split_cve[1]
                    numerical_cve = split_cve[2]
                    for _ in range(2):
                        if numerical_cve.startswith('0'):
                            numerical_cve = numerical_cve[1:]
                    filenames.append(cve_begin + '-' + numerical_cve)
                except:
                    pass
        return filenames

    def save_cve_data(self, json_data):
        cve = json_data['cveMetadata']['cveId']
        split_cve = cve.split('-')
        year = split_cve[1]
        cve_padded = str('{:06d}'.format(int(split_cve[2])))
        year_path = os.path.join(self.path_storage, year)
        if not os.path.exists(year_path):
            os.makedirs(year_path)
        two_digits_path = os.path.join(year_path, cve_padded[:2])
        if not os.path.exists(two_digits_path):
            os.makedirs(two_digits_path)
        one_digit_path = os.path.join(two_digits_path, cve_padded[2:4])
        if not os.path.exists(one_digit_path):
            os.makedirs(one_digit_path)
        with open(os.path.join(one_digit_path, f'CVE-{year}-{cve_padded}.json'), 'w', encoding='utf-8') as file:
            file.write(json.dumps(json_data))

    def add_references_to_json(self, json_data):
        references = []
        try:
            for ref in json_data['containers']['cna']['references']:
                references.append(ref['url'])
            read_references = []
            for ref_url in references:
                try:
                    response = requests.get(ref_url, timeout=self.request_timeout)
                    if response.status_code == 200:
                        read_references.append((ref_url, response.text))
                    else:
                        read_references.append((ref_url, response.status_code))
                except:
                    read_references.append((ref_url, "Error with the request"))
            json_data['added_references'] = read_references
        except:
            pass
        return json_data
