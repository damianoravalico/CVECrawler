import json
import logging
import os
import time
import requests
import datetime


class CVECrawler:
    ENDPOINT_NIST = 'https://services.nvd.nist.gov/rest/json/cves/2.0'
    INDEX_FILENAME = '.index.txt'
    LAST_UPDATE_FILENAME = '.last_timestamp.txt'

    def __init__(self,
                 path_storage='/Users/dravalico/PycharmProjects/crawlers/cve-crawler/cve',
                 request_timeout=25,
                 interval_between_requests=6,  # Suggested by NIST
                 update_interval=7200,  # Suggested by NIST
                 retry_interval=30,
                 retries_for_request=9):
        self.path_storage = path_storage
        self.request_timeout = request_timeout
        self.interval_between_requests = interval_between_requests
        self.update_interval = update_interval
        self.retry_interval = retry_interval
        self.retries_for_request = retries_for_request
        log_format = f'[%(asctime)s] [%(levelname)s] %(message)s'
        logging.basicConfig(level=logging.INFO, format=log_format, datefmt="%Y-%m-%d %H:%M:%S")

    def run(self):
        logging.info("Crawler up")
        if not os.path.exists(self.path_storage):
            os.makedirs(self.path_storage)
        logging.info('Initialisation of the data population')
        self.init_data_population()
        logging.info('Initialisation completed')
        while True:
            logging.info(f'Starting the cycle...')
            self.maintain_data()
            logging.info(f'Going to sleep for {self.update_interval} seconds due to normal stand-by mode')
            time.sleep(self.update_interval)
            logging.info('Crawler woke up from stand-by mode')

    def init_data_population(self):
        try:
            with open(os.path.join(self.path_storage, self.INDEX_FILENAME), 'r') as file:
                index = int(file.read())
        except:
            index = 0
        entries_for_request = 2000
        actual_retries = 0
        while True:
            with open(os.path.join(self.path_storage, self.INDEX_FILENAME), 'w') as file:
                file.write(str(index))
            is_exception_or_too_many_request = False
            query = f'?startIndex={index}'
            url = self.ENDPOINT_NIST + query
            logging.info(f'Request for {entries_for_request} entries from index={index}')
            try:
                response = requests.get(url, timeout=self.request_timeout)
                if response.status_code == 200:
                    logging.info(f'Data obtained for {entries_for_request} entries from index={index}')
                    response_json = response.json()
                    if response_json['startIndex'] >= response_json['totalResults']:
                        logging.info('Data up to date')
                        break
                    self.save_timestamp_and_add_references(response_json)
                    logging.info(f'Data saved for {entries_for_request} entries from index={index}')
                    index += entries_for_request
                    actual_retries = 0
                else:
                    logging.error(f'Cannot obtain data for index={index}, status code={response.status_code}')
                    if response.status_code == 429 or response.status_code == 503:
                        is_exception_or_too_many_request = True
                    actual_retries += 1
                    if actual_retries == self.retries_for_request:
                        actual_retries = 0
                        index += entries_for_request
            except Exception as e:
                logging.exception(e)
                is_exception_or_too_many_request = True
            if is_exception_or_too_many_request:
                logging.info(
                    f'Going to sleep for {self.retry_interval} seconds due to too many requests or an exception')
                time.sleep(self.retry_interval)
            else:
                logging.info(f'Going to sleep for {self.interval_between_requests} seconds before the next request')
                time.sleep(self.interval_between_requests)
            logging.info('Crawler woke up')

    def save_timestamp_and_add_references(self, response_json):
        with open(os.path.join(self.path_storage, self.LAST_UPDATE_FILENAME), 'w') as file:
            file.write(response_json['timestamp'])
        json_list = response_json['vulnerabilities']
        logging.info("Adding raw references to items")
        for e in json_list:
            complete_json = self.add_references_to_json(e)
            self.save_cve_data(complete_json)

    @staticmethod
    def add_references_to_json(json_data):
        references = []
        try:
            for ref in json_data['cve']['references']:
                references.append(ref['url'])
            read_references = []
            for ref_url in references:
                try:
                    response = requests.get(ref_url, timeout=3)
                    if response.status_code == 200:
                        read_references.append((ref_url, response.text))
                    else:
                        read_references.append((ref_url, response.status_code))
                except:
                    read_references.append((ref_url, "Error with the request"))
            json_data['cve']['added_references'] = read_references
        except:
            pass
        return json_data

    def save_cve_data(self, json_data):
        try:
            cve = json_data['cve']['id']
            split_cve = cve.split('-')
            year = split_cve[1]
            cve_padded = str('{:06d}'.format(int(split_cve[2])))
            year_path = os.path.join(self.path_storage, year)
            if not os.path.exists(year_path):
                os.makedirs(year_path)
            two_digits_path = os.path.join(year_path, cve_padded[:2])
            if not os.path.exists(two_digits_path):
                os.makedirs(two_digits_path)
            one_digit_path = os.path.join(two_digits_path, cve_padded[2:4])
            if not os.path.exists(one_digit_path):
                os.makedirs(one_digit_path)
            with open(os.path.join(one_digit_path, f'CVE-{year}-{cve_padded}.json'), 'w') as file:
                file.write(json.dumps(json_data))
        except:
            raise RuntimeError('Cannot save data')

    def maintain_data(self):
        try:
            with open(os.path.join(self.path_storage, self.LAST_UPDATE_FILENAME), 'r') as file:
                timestamp = file.read()
        except:
            logging.exception('Now last timestamp detected, creating a new one with current time')
            timestamp = str(datetime.datetime.now()).replace(' ', '')
            with open(os.path.join(self.path_storage, self.LAST_UPDATE_FILENAME), 'w') as file:
                file.write(timestamp)
        logging.info(f'Request for update local data from {timestamp}')
        now = str(datetime.datetime.now()).replace(' ', '')
        query = f'?lastModStartDate={timestamp}&lastModEndDate={now}'
        url = self.ENDPOINT_NIST + query
        try:
            response = requests.get(url, timeout=self.request_timeout)
            if response.status_code == 200:
                logging.info(f'Data obtained from {timestamp} to {now}')
                response_json = response.json()
                self.save_timestamp_and_add_references(response_json)
                logging.info(f'Data saved from {timestamp} to {now}')
            else:
                logging.error(f'Cannot obtain data from {timestamp} to {now}')
        except Exception as e:
            logging.exception(e)
