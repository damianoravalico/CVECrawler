import logging
import os
import time
import requests
import xml.etree.ElementTree as ET
from bs4 import BeautifulSoup
import json


class ExploitDbCrawler:
    def __init__(self,
                 storage_path='/usr/src/data',
                 request_timeout=60,
                 interval_between_requests=6,
                 update_interval=86400,
                 retry_interval=1200):
        self.storage_path = storage_path
        self.request_timeout = request_timeout
        self.interval_between_requests = interval_between_requests
        self.update_interval = update_interval
        self.retry_interval = retry_interval
        log_format = f'[%(asctime)s] [%(levelname)s] %(message)s'
        logging.basicConfig(level=logging.INFO, format=log_format, datefmt='%Y-%m-%d %H:%M:%S')

    def run(self):
        logging.info('Crawler up')
        os.makedirs(self.storage_path, exist_ok=True)
        endpoint_exploits = 'https://www.exploit-db.com/exploits/'
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) '
                          'Chrome/58.0.3029.110 Safari/537.3'
        }
        missing_element_from_error = 0
        while True:
            logging.info(f'Starting the cycle...')
            try:
                last_online_id = self.retrieve_last_edb_id_from_rss()
                last_local_id = self.retrieve_last_local_edb_id()
                logging.info(f'The last local edb-id is {last_local_id} and the last online EDB-ID is {last_online_id}')
                edb_ids = [e for e in range(last_local_id + 1, last_online_id + 1)]
                if missing_element_from_error != 0:
                    edb_ids.insert(0, missing_element_from_error)
                    missing_element_from_error = 0
                for edb_id in edb_ids:
                    logging.info(f'Request for EDB-ID={edb_id}')
                    json_data = {}
                    try:
                        response = requests.get(endpoint_exploits + str(edb_id),
                                                timeout=self.request_timeout,
                                                headers=headers)
                        if response.status_code == 200:
                            soup = BeautifulSoup(response.text, 'html.parser')
                            info_titles = soup.find_all('h4', class_='info-title')
                            info_titles = [e.text.strip().replace(':', '') for e in info_titles]
                            stats_titles = soup.find_all('h6', class_='stats-title')
                            stats_titles = [e.text.strip() for e in stats_titles]
                            for i, s in zip(info_titles, stats_titles):
                                json_data[f'{i}'] = s
                            soup = BeautifulSoup(response.text, 'html.parser')
                            code_tag = soup.find('code')
                            json_data['code'] = code_tag.get_text()
                            logging.info(f'Data obtained for EDB-ID={edb_id}')
                            self.save_edb_id_data(json_data)
                            logging.info(f'Data saved for EDB-ID={edb_id}')
                        else:
                            logging.error(f'Cannot obtain data for EDB-ID={edb_id}, status code={response.status_code}')
                            if response.status_code != 404:
                                missing_element_from_error = edb_id
                                logging.info(
                                    f'Going to sleep for {self.retry_interval} seconds due to too many requests')
                                time.sleep(self.retry_interval)
                    except Exception as e:
                        logging.exception(e)
                    finally:
                        logging.info(
                            f'Going to sleep for {self.interval_between_requests} seconds before the next request')
                        time.sleep(self.interval_between_requests)
            except Exception as e:
                logging.exception(e)
            logging.info(f'Going to sleep for {self.update_interval} seconds due to normal stand-by mode')
            time.sleep(self.update_interval)
            logging.info('Crawler woke up')

    def retrieve_last_edb_id_from_rss(self):
        rss_url = 'https://www.exploit-db.com/rss.xml'
        try:
            response = requests.get(rss_url, timeout=self.request_timeout)
            if response.status_code == 200:
                xml_data = ET.fromstring(response.content)
                return int(xml_data.find('.//guid').text.split('/')[-1])
            else:
                raise RuntimeError(f'Cannot retrieve rss feed. Status code: {response.status_code}')
        except:
            raise RuntimeError('Cannot retrieve rss feed')

    def retrieve_last_local_edb_id(self):
        filenames = []
        for root_folder, folders, files in os.walk(self.storage_path):
            for file in files:
                edb_id = os.path.join(root_folder, file)
                try:
                    edb_id = edb_id.split('/')[-1].replace('.json', '')
                    filenames.append(int(edb_id))
                except:
                    pass
        return max(filenames) if filenames != [] else 0

    def save_edb_id_data(self, json_data):
        try:
            date = json_data['Date'].split('-')
            year = date[0]
            month = date[1]
            full_path = os.path.join(self.storage_path, year, month)
            os.makedirs(full_path, exist_ok=True)
            with open(os.path.join(full_path, f'{json_data["EDB-ID"]}.json'), 'w') as file:
                file.write(json.dumps(json_data))
        except:
            raise RuntimeError('Cannot save data')


if __name__ == '__main__':
    ExploitDbCrawler().run()
